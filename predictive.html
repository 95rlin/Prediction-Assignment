<h1>Prediction-Assignment</h1>
<h3><a id="user-content-introduction" class="anchor" href="https://github.com/95rlin/Prediction-Assignment/blob/main/README.md#introduction" aria-hidden="true"></a>Introduction</h3>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement &ndash; a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways</p>
<p>Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions:</p>
<ol>
<li>exactly according to the specification (Class A),</li>
<li>throwing the elbows to the front (Class B)</li>
<li>lifting the dumbbell only halfway (Class C)</li>
<li>lowering the dumbbell only halfway (Class D)</li>
<li>throwing the hips to the front (Class E)</li>
</ol>
<h3><a id="user-content-preparing-datasets" class="anchor" href="https://github.com/95rlin/Prediction-Assignment/blob/main/README.md#preparing-datasets" aria-hidden="true"></a>Preparing Datasets</h3>
<p>The datasets are imported. All of the non-numerical data (blank spaces, "NA", "#DIV/0!") is set to NA.</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>library(caret)
library(randomForest)

train_csv &lt;- read.csv(file = "week4_proj/pml-training.csv", header = TRUE, na.strings=c("", "NA", "#DIV/0!"))
test_csv &lt;- read.csv(file = "week4_proj/pml-testing.csv", header = TRUE, na.strings=c("", "NA", "#DIV/0!")) 
</code></pre>
</div>
<p>The datasets are further cleaned by removing the useless data in the first 7 columns. Columns with NA are also removed.</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>train_csv &lt;- train_csv[ , -c(1:7)] # remove first 7 columns of useless data
test_csv &lt;- test_csv[ , -c(1:7)] # remove first 7 columns of useless data
clean_train_csv &lt;- train_csv[, colSums(is.na(train_csv)) ==0]] # remove columns with NA
clean_test_csv &lt;- test_csv[,colSums(is.na(test_csv)) == 0] # remove columns with NA
</code></pre>
</div>
<p>Clean_train_csv is partitioned 70% for training and 30% for testing. The seed is set to 999.</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>set.seed(999)
inTrain &lt;- createDataPartition(y=clean_train_csv$classe, p=0.7, list=FALSE) #
training &lt;- clean_train_csv[inTrain,]
testing &lt;- clean_train_csv[-inTrain,]
</code></pre>
</div>
<h3><a id="user-content-model-random-forest" class="anchor" href="https://github.com/95rlin/Prediction-Assignment/blob/main/README.md#model-random-forest" aria-hidden="true"></a>Model (Random Forest)</h3>
<p>I decided to use Random Forest as the model because of its accuracy. However, I had to limit the number of trees generated in the model because of Random Forests's slow speed. The accuracy of Random Forest can also be improved by implementing a k-fold cross validation. A 2nd modfit is generated with 10 folds in a grid search.</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>modFit &lt;- train(classe ~., data=clean_train_csv, method="rf", ntree=5)

trControl &lt;- trainControl(method = "cv", number=10, search="grid")
modFit2 &lt;- train(classe ~., data=clean_train_csv, trControl = trControl, method="rf", ntree=5)
</code></pre>
</div>
<h3><a id="user-content-validation" class="anchor" href="https://github.com/95rlin/Prediction-Assignment/blob/main/README.md#validation" aria-hidden="true"></a>Validation</h3>
<p>I use the fitted model in Modfit and Modfit2 to test the prediction accuracy with the testing set. To calculate the accuracy, I divided each prediction's number of correct guesses over the total number of test cases (testing's number of rows). It was found that modFit2 has better accuracy at 99.98% compared to modFit's 99.81%. Although both models are probably accurate enough to use.</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>pred &lt;- predict(modFit, testing)
&gt; table(pred,testing$classe)
pred    A    B    C    D    E
   A 1674    0    0    0    0
   B    0 1139    2    0    1
   C    0    0 1024    2    1
   D    0    0    0  962    0
   E    0    0    0    0 1080
pred2 &lt;- predict(modFit2, testing)
&gt; table(pred2,testing$classe)
pred2    A    B    C    D    E
    A 1674    1    0    0    0
    B    0 1138    0    0    0
    C    0    0 1026    0    0
    D    0    0    0  964    0
    E    0    0    0    0 1082

accuracy1 &lt;- sum(pred==testing$classe)/(nrow(testing))
&gt; accuracy1
[1] 0.9981308
accuracy2 &lt;- sum(pred2==testing$classe)/(nrow(testing))
&gt; accuracy2
[1] 0.9998301
</code></pre>
</div>
<h3><a id="user-content-prediction" class="anchor" href="https://github.com/95rlin/Prediction-Assignment/blob/main/README.md#prediction" aria-hidden="true"></a>Prediction</h3>
<p>Since modFit2 is more accurate, I use this model to predict the test set's classe (clean_test_csv).</p>
<div class="snippet-clipboard-content position-relative">
<pre><code>results &lt;- predict(modFit2, newdata=clean_test_csv)
&gt; results
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E</code></pre>
</div>
